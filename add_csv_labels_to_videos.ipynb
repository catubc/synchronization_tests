{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "%matplotlib tk\n",
    "%autosave 180\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import gridspec\n",
    "from scipy import signal\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "\n",
    "#import glob2\n",
    "\n",
    "from numba import jit\n",
    "import tables\n",
    "from scipy.io import loadmat\n",
    "import scipy\n",
    "import h5py\n",
    "#import hdf5storage\n",
    "import csv\n",
    "\n",
    "import deeplabcut\n",
    "\n",
    "colors = [\n",
    "'black','grey','brown','slategrey','darkviolet','darkmagenta',\n",
    "'blue','blue','blue',\n",
    "'red','red','red',\n",
    "'green','green','green',\n",
    "'cyan','cyan','cyan',\n",
    "'orange','orange','orange',\n",
    "    \n",
    "'orange','firebrick','lawngreen','dodgerblue','crimson','orchid','slateblue',\n",
    "'darkgreen','darkorange','indianred','darkviolet','deepskyblue','greenyellow',\n",
    "'peru','cadetblue','forestgreen','slategrey','lightsteelblue','rebeccapurple',\n",
    "'darkmagenta','yellow','hotpink']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['df_with_missing']>\n",
      "<KeysViewHDF5 ['_i_table', 'table']>\n",
      "4500\n",
      "(1000, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n"
     ]
    }
   ],
   "source": [
    "# LOAD LABELS FROM .BX.H5 FILE\"\n",
    "fname = '/media/cat/14TB/insync_cm5636/march_2/video/dlc_2.26b_results/march_2_redo-cat-2020-06-17/videos/2020-3-8_12_08_57_943006_compressed_3min_sameDLC_resnet50_march_2_redoJun17shuffle1_50000_bx.h5'\n",
    "#fname = '/media/cat/4TBSSD/Downloads/2020-3-8_12_08_57_943006_compressed_3min_sameDLC_resnet50_cat3Jun12shuffle1_50000_bx.h5'\n",
    "hf = h5py.File(fname, 'r')\n",
    "print (hf.keys())\n",
    "data = hf.get('df_with_missing')\n",
    "print (data.keys())\n",
    "table = data.get('table')\n",
    "print (len(table))\n",
    "# itable = data.get('_i_table').get('index')\n",
    "# bbounds = itable.get('zbounds')\n",
    "print (table[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([639, 638, 637, 636, 642, 643, 644, 645, 652, 654, 647, 650, 646, 655, 658, 659, 660, 668, 665, 674, 675, 678, 682, 684, 686, 688, 689, 690, 694, 697, 699, 704, 710, 711, 716, 715, 718, 722, 724, 727, 728, 731, 732, 734, 736, 737, 738, 735, 739, 743, 741, 748, 749, 750, 753, 755, 758, 756, 767, 774, 768, 778, 781, 780, 787, 786, 789, 785, 790, 793, 792, 794, 796, 800, 812, 818, 817, 819, 827, 830, 834, 826, 837, 840, 829, 843, 844, 846, 841, 849, 852, 854, 857, 855, 861, 863, 868, 870, 871, 875, 887, 893, 894, 900, 904, 903, 905, 906, 909, 910, 915, 924, 935, 'header'])\n"
     ]
    }
   ],
   "source": [
    "#fname = '/media/cat/4TBSSD/Downloads/2020-3-8_12_08_57_943006_compressed_3min_sameDLC_resnet50_march_2_redoJun17shuffle1_50000_full.pickle'\n",
    "fname1 = '/media/cat/14TB/insync_cm5636/march_2/video/dlc_2.26b_results/march_2_redo-cat-2020-06-17/videos/2020-3-8_12_08_57_943006_compressed_3min_sameDLC_resnet50_march_2_redoJun17shuffle1_50000_full.pickle'\n",
    "fname2 = '/media/cat/14TB/insync_cm5636/march_2/video/dlc_2.26b_results/march_2_redo-cat-2020-06-17/videos/new/2020-3-8_12_08_57_943006_compressed_3min_sameDLC_resnet50_march_2_redoJun17shuffle1_50000_bx.pickle'\n",
    "\n",
    "data_pickle = np.load(fname2,allow_pickle=True)\n",
    "print (data_pickle.keys())\n",
    "#print (data_pickle['data']['cropping_parameters'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found output file for scorer: DLC_resnet50_march_2_redoJun17shuffle1_50000_bx_filtered\n",
      "Converting to csv...\n",
      "Found output file for scorer: DLC_resnet50_march_2_redoJun17shuffle1_50000_bx\n",
      "Converting to csv...\n",
      "All pose files were converted.\n"
     ]
    }
   ],
   "source": [
    "path_to_video = '/media/cat/14TB/insync_cm5636/march_2/video/dlc_2.26b_results/march_2_redo-cat-2020-06-17/videos/new/'\n",
    "deeplabcut.analyze_videos_converth5_to_csv(path_to_video, '.avi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'frame0000'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-65cfd9cfccfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata_pickle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'frame0000'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# for k in range(14):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#     print (np.array(data_pickle['frame0000']['coordinates'][0][k]).shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     print (data_pickle['frame0000']['confidence'][0].squeeze().shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'frame0000'"
     ]
    }
   ],
   "source": [
    "print (data_pickle['frame0000'].keys())\n",
    "# for k in range(14):\n",
    "#     print (np.array(data_pickle['frame0000']['coordinates'][0][k]).shape)\n",
    "#     print (data_pickle['frame0000']['confidence'][0].squeeze().shape)\n",
    "\n",
    "print (data_pickle['frame0000']['costs'].keys())\n",
    "print (data_pickle['frame0000']['costs'][0].keys())\n",
    "for k in range(len(data_pickle['frame0000']['costs'])):\n",
    "    print (data_pickle['frame0000']['costs'][k]['m1'].shape)\n",
    "    print (data_pickle['frame0000']['costs'][k]['distance'].shape)\n",
    "    print (\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/cat/14TB/insync_cm5636/march_2/video/dlc_2.26b_results/march_2_redo-cat-2020-06-17/videos/new/  already exists!\n",
      "Detections already plotted,  /media/cat/14TB/insync_cm5636/march_2/video/dlc_2.26b_results/march_2_redo-cat-2020-06-17/videos/new/2020-3-8_12_08_57_943006_compressed_3min_sameDLC_resnet50_march_2_redoJun17shuffle1_50000_full.mp4\n"
     ]
    }
   ],
   "source": [
    "def create_video_with_all_detections(config, videos, DLCscorername, destfolder=None):\n",
    "    \"\"\"\n",
    "    Create a video labeled with all the detections stored in a '*_full.pickle' file.\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : str\n",
    "        Absolute path to the config.yaml file\n",
    "    videos : list of str\n",
    "        A list of strings containing the full paths to videos for analysis or a path to the directory,\n",
    "        where all the videos with same extension are stored.\n",
    "    DLCscorername: str\n",
    "        Name of network. E.g. 'DLC_resnet50_project_userMar23shuffle1_50000\n",
    "    destfolder: string, optional\n",
    "        Specifies the destination folder that was used for storing analysis data (default is the path of the video).\n",
    "    \"\"\"\n",
    "    from deeplabcut.pose_estimation_tensorflow.lib.inferenceutils import (\n",
    "        convertdetectiondict2listoflist,\n",
    "    )\n",
    "    import pickle, re\n",
    "    from deeplabcut import auxiliaryfunctions\n",
    "    from pathlib import Path\n",
    "    from tqdm import trange\n",
    "    from skimage.draw import circle, line_aa\n",
    "    \n",
    "    from deeplabcut.utils.video_processor import (\n",
    "    VideoProcessorCV as vp)  # used to CreateVideo\n",
    "    \n",
    "    cfg = auxiliaryfunctions.read_config(config)\n",
    "\n",
    "    out_array = []\n",
    "    \n",
    "    for video in videos:\n",
    "        videofolder = os.path.splitext(video)[0]\n",
    "\n",
    "        if destfolder is None:\n",
    "            outputname = \"{}_full.mp4\".format(videofolder + DLCscorername)\n",
    "            full_pickle = os.path.join(videofolder + DLCscorername + \"_full.pickle\")\n",
    "        else:\n",
    "            auxiliaryfunctions.attempttomakefolder(destfolder)\n",
    "            outputname = os.path.join(\n",
    "                destfolder, str(Path(video).stem) + DLCscorername + \"_full.mp4\"\n",
    "            )\n",
    "            full_pickle = os.path.join(\n",
    "                destfolder, str(Path(video).stem) + DLCscorername + \"_full.pickle\"\n",
    "            )\n",
    "\n",
    "        if not (os.path.isfile(outputname)):\n",
    "            print(\"Creating labeled video for \", str(Path(video).stem))\n",
    "            with open(full_pickle, \"rb\") as file:\n",
    "                data = pickle.load(file)\n",
    "\n",
    "            header = data.pop(\"metadata\")\n",
    "            all_jointnames = header[\"all_joints_names\"]\n",
    "\n",
    "            numjoints = len(all_jointnames)\n",
    "            bpts = range(numjoints)\n",
    "            frame_names = list(data)\n",
    "            frames = [int(re.findall(r\"\\d+\", name)[0]) for name in frame_names]\n",
    "            colorclass = plt.cm.ScalarMappable(cmap=cfg[\"colormap\"])\n",
    "            C = colorclass.to_rgba(np.linspace(0, 1, numjoints))\n",
    "            colors = (C[:, :3] * 255).astype(np.uint8)\n",
    "            print (\"COLORSL \", colors)\n",
    "\n",
    "            pcutoff = cfg[\"pcutoff\"]\n",
    "            dotsize = cfg[\"dotsize\"]\n",
    "            clip = vp(fname=video, sname=outputname, codec=\"mp4v\")\n",
    "            ny, nx = clip.height(), clip.width()\n",
    "\n",
    "            print (\"clip.nframes: \", clip.nframes)\n",
    "            print (\"frames: \", len(frames))\n",
    "            print (\"pcutoff: \", pcutoff)\n",
    "            pcutoff = 0\n",
    "            for n in trange(clip.nframes):\n",
    "                frame = clip.load_frame()\n",
    "                try:\n",
    "                    ind = frames.index(n)\n",
    "                    dets = convertdetectiondict2listoflist(data[frame_names[ind]], bpts)\n",
    "                    \n",
    "                    print (n, \" len dets: \", len(dets))\n",
    "                    print (dets[0])\n",
    "                    for i, det in enumerate(dets):\n",
    "                        color = colors[i]\n",
    "                        for x, y, p, _ in det:\n",
    "                            print (i, \" x, y, p:\", x, y, p)\n",
    "                            if p > pcutoff:\n",
    "                                rr, cc = circle(y, x, dotsize, shape=(ny, nx))\n",
    "                                frame[rr, cc] = color\n",
    "                        break\n",
    "                except ValueError:  # No data stored for that particular frame\n",
    "                    print(n, \"no data\")\n",
    "                    pass\n",
    "                \n",
    "                try:\n",
    "                    clip.save_frame(frame)\n",
    "                except:\n",
    "                    print(n, \"frame writing error.\")\n",
    "                    pass\n",
    "            clip.close()\n",
    "        else:\n",
    "            print(\"Detections already plotted, \", outputname)\n",
    "            \n",
    "config = '/media/cat/14TB/insync_cm5636/march_2/video/dlc_2.26b_results/march_2_redo-cat-2020-06-17/config.yaml'\n",
    "videos = ['/media/cat/14TB/insync_cm5636/march_2/video/dlc_2.26b_results/march_2_redo-cat-2020-06-17/videos/new/2020-3-8_12_08_57_943006_compressed_3min_same.avi']\n",
    "DLCscorername = 'DLC_resnet50_march_2_redoJun17shuffle1_50000'\n",
    "destfolder = '/media/cat/14TB/insync_cm5636/march_2/video/dlc_2.26b_results/march_2_redo-cat-2020-06-17/videos/new/'\n",
    "create_video_with_all_detections(config, videos, DLCscorername, destfolder)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path_test_config:  /media/cat/14TB/insync_cm5636/march_2/video/dlc_2.26b_results/march_2_redo-cat-2020-06-17/dlc-models/iteration-0/march_2_redoJun17-trainset95shuffle1/test/pose_cfg.yaml\n",
      "Snapshots:  ['snapshot-48000' 'snapshot-49000' 'snapshot-49500' 'snapshot-48500'\n",
      " 'snapshot-50000']\n",
      "increasing_indices:  [0 3 1 2 4]\n",
      "modelfolder:  /media/cat/14TB/insync_cm5636/march_2/video/dlc_2.26b_results/march_2_redo-cat-2020-06-17/dlc-models/iteration-0/march_2_redoJun17-trainset95shuffle1\n",
      "Using snapshot-50000 for model /media/cat/14TB/insync_cm5636/march_2/video/dlc_2.26b_results/march_2_redo-cat-2020-06-17/dlc-models/iteration-0/march_2_redoJun17-trainset95shuffle1\n",
      "Processing...  /media/cat/14TB/insync_cm5636/march_2/video/dlc_2.26b_results/march_2_redo-cat-2020-06-17/videos/new/2020-3-8_12_08_57_943006_compressed_3min_same.avi\n",
      "/media/cat/14TB/insync_cm5636/march_2/video/dlc_2.26b_results/march_2_redo-cat-2020-06-17/videos/new  already exists!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/cat/14TB/insync_cm5636/march_2/video/dlc_2.26b_results/march_2_redo-cat-2020-06-17/videos/new/2020-3-8_12_08_57_943006_compressed_3min_sameDLC_resnet50_march_2_redoJun17shuffle1_50000_full.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7319d4f3deb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0mmodelprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0mtrack_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"box\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m     edgewisecondition=True)\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-7319d4f3deb8>\u001b[0m in \u001b[0;36mconvert_detections2tracklets\u001b[0;34m(config, videos, videotype, shuffle, trainingsetindex, overwrite, destfolder, BPTS, iBPTS, PAF, printintermediate, inferencecfg, modelprefix, track_method, edgewisecondition)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mvname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mdataname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideofolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mDLCscorer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauxfun_multianimal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadFullMultiAnimalData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sk\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrack_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"skeleton\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"bx\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mtrackname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf\"_{method}.pickle\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DLC-GPU/lib/python3.7/site-packages/deeplabcut/utils/auxfun_multianimal.py\u001b[0m in \u001b[0;36mLoadFullMultiAnimalData\u001b[0;34m(dataname)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mLoadFullMultiAnimalData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;34m\"\"\" Save predicted data as h5 file and metadata as pickle file; created by predict_videos.py \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_full.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_meta.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/cat/14TB/insync_cm5636/march_2/video/dlc_2.26b_results/march_2_redo-cat-2020-06-17/videos/new/2020-3-8_12_08_57_943006_compressed_3min_sameDLC_resnet50_march_2_redoJun17shuffle1_50000_full.pickle'"
     ]
    }
   ],
   "source": [
    "\n",
    "def convert_detections2tracklets(\n",
    "    config,\n",
    "    videos,\n",
    "    videotype=\"avi\",\n",
    "    shuffle=1,\n",
    "    trainingsetindex=0,\n",
    "    overwrite=False,\n",
    "    destfolder=None,\n",
    "    BPTS=None,\n",
    "    iBPTS=None,\n",
    "    PAF=None,\n",
    "    printintermediate=False,\n",
    "    inferencecfg=None,\n",
    "    modelprefix=\"\",\n",
    "    track_method=\"box\",\n",
    "    edgewisecondition=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    This should be called at the end of deeplabcut.analyze_videos for multianimal projects!\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : string\n",
    "        Full path of the config.yaml file as a string.\n",
    "    videos : list\n",
    "        A list of strings containing the full paths to videos for analysis or a path to the directory, where all the videos with same extension are stored.\n",
    "    videotype: string, optional\n",
    "        Checks for the extension of the video in case the input to the video is a directory.\\n Only videos with this extension are analyzed. The default is ``.avi``\n",
    "    shuffle: int, optional\n",
    "        An integer specifying the shuffle index of the training dataset used for training the network. The default is 1.\n",
    "    trainingsetindex: int, optional\n",
    "        Integer specifying which TrainingsetFraction to use. By default the first (note that TrainingFraction is a list in config.yaml).\n",
    "    overwrite: bool, optional.\n",
    "        Overwrite tracks file i.e. recompute tracks from full detections and overwrite.\n",
    "    destfolder: string, optional\n",
    "        Specifies the destination folder for analysis data (default is the path of the video). Note that for subsequent analysis this\n",
    "        folder also needs to be passed.\n",
    "    track_method: str, optional\n",
    "        Method uses to track animals, either 'box' or 'skeleton'.\n",
    "        By default, a constant velocity Kalman filter is used to track individual bounding boxes.\n",
    "    BPTS: Default is None: all bodyparts are used.\n",
    "        Pass list of indices if only certain bodyparts should be used (advanced).\n",
    "    iBPTS: Default is None: all bodyparts are used.\n",
    "        The inverse indices from BPTS.\n",
    "        TODO: calculate from BPTS\n",
    "    PAF: Default is None: all connections are used.\n",
    "        Pass list of indices if only certain connections should be used (advanced).\n",
    "    printintermediate: ## TODO\n",
    "        Default is false.\n",
    "    inferencecfg: Default is None.\n",
    "        Configuaration file for inference (assembly of individuals). Ideally\n",
    "        should be optained from cross validation (during evaluation). By default\n",
    "        the parameters are loaded from inference_cfg.yaml, but these get_level_values\n",
    "        can be overwritten.\n",
    "    edgewisecondition: bool, default False.\n",
    "        If true pairwise Euclidean distances of limbs (connections in PAF) will be\n",
    "        estimated from the annotated data and used for excluding possible connections.\n",
    "    Examples\n",
    "    --------\n",
    "    If you want to convert detections to tracklets:\n",
    "    >>> deeplabcut.convert_detections2tracklets('/analysis/project/reaching-task/config.yaml',[]'/analysis/project/video1.mp4'], videotype='.mp4')\n",
    "    --------\n",
    "    \"\"\"\n",
    "    from deeplabcut.pose_estimation_tensorflow.lib import inferenceutils, trackingutils\n",
    "    from deeplabcut.utils import auxfun_multianimal\n",
    "    from easydict import EasyDict as edict\n",
    "    import pickle\n",
    "\n",
    "    if track_method not in (\"box\", \"skeleton\"):\n",
    "        raise ValueError(\n",
    "            \"Invalid tracking method. Only `box` and `skeleton` are currently supported.\"\n",
    "        )\n",
    "\n",
    "    cfg = auxiliaryfunctions.read_config(config)\n",
    "    trainFraction = cfg[\"TrainingFraction\"][trainingsetindex]\n",
    "    start_path = os.getcwd()  # record cwd to return to this directory in the end\n",
    "\n",
    "    # TODO: addd cropping as in video analysis!\n",
    "    # if cropping is not None:\n",
    "    #    cfg['cropping']=True\n",
    "    #    cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping\n",
    "    #    print(\"Overwriting cropping parameters:\", cropping)\n",
    "    #    print(\"These are used for all videos, but won't be save to the cfg file.\")\n",
    "\n",
    "    modelfolder = os.path.join(\n",
    "        cfg[\"project_path\"],\n",
    "        str(\n",
    "            auxiliaryfunctions.GetModelFolder(\n",
    "                trainFraction, shuffle, cfg, modelprefix=modelprefix\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "    path_test_config = Path(modelfolder) / \"test\" / \"pose_cfg.yaml\"\n",
    "    try:\n",
    "        dlc_cfg = load_config(str(path_test_config))\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\n",
    "            \"It seems the model for shuffle %s and trainFraction %s does not exist.\"\n",
    "            % (shuffle, trainFraction)\n",
    "        )\n",
    "\n",
    "    print (\"path_test_config: \", path_test_config)\n",
    "    if \"multi-animal\" not in dlc_cfg[\"dataset_type\"]:\n",
    "        raise ValueError(\"This function is only required for multianimal projects!\")\n",
    "\n",
    "    path_inference_config = Path(modelfolder) / \"test\" / \"inference_cfg.yaml\"\n",
    "    if inferencecfg is None:  # then load or initialize\n",
    "        inferencecfg = auxfun_multianimal.read_inferencecfg(path_inference_config, cfg)\n",
    "    else:\n",
    "        inferencecfg = edict(inferencecfg)\n",
    "        auxfun_multianimal.check_inferencecfg_sanity(cfg, inferencecfg)\n",
    "\n",
    "    if edgewisecondition:\n",
    "        path_inferencebounds_config = (\n",
    "            Path(modelfolder) / \"test\" / \"inferencebounds.yaml\"\n",
    "        )\n",
    "        try:\n",
    "            inferenceboundscfg = auxiliaryfunctions.read_plainconfig(\n",
    "                path_inferencebounds_config\n",
    "            )\n",
    "        except FileNotFoundError:\n",
    "            print(\"Computing distances...\")\n",
    "            from deeplabcut.pose_estimation_tensorflow import calculatepafdistancebounds\n",
    "\n",
    "            inferenceboundscfg = calculatepafdistancebounds(\n",
    "                config, shuffle, trainingsetindex\n",
    "            )\n",
    "            auxiliaryfunctions.write_plainconfig(\n",
    "                path_inferencebounds_config, inferenceboundscfg\n",
    "            )\n",
    "\n",
    "    # Check which snapshots are available and sort them by # iterations\n",
    "    try:\n",
    "        Snapshots = np.array(\n",
    "            [\n",
    "                fn.split(\".\")[0]\n",
    "                for fn in os.listdir(os.path.join(modelfolder, \"train\"))\n",
    "                if \"index\" in fn\n",
    "            ]\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\n",
    "            \"Snapshots not found! It seems the dataset for shuffle %s has not been trained/does not exist.\\n Please train it before using it to analyze videos.\\n Use the function 'train_network' to train the network for shuffle %s.\"\n",
    "            % (shuffle, shuffle)\n",
    "        )\n",
    "\n",
    "    if cfg[\"snapshotindex\"] == \"all\":\n",
    "        print(\n",
    "            \"Snapshotindex is set to 'all' in the config.yaml file. Running video analysis with all snapshots is very costly! Use the function 'evaluate_network' to choose the best the snapshot. For now, changing snapshot index to -1!\"\n",
    "        )\n",
    "        snapshotindex = -1\n",
    "    else:\n",
    "        snapshotindex = cfg[\"snapshotindex\"]\n",
    "\n",
    "    print (\"Snapshots: \", Snapshots)\n",
    "    increasing_indices = np.argsort([int(m.split(\"-\")[1]) for m in Snapshots])\n",
    "    print (\"increasing_indices: \", increasing_indices)\n",
    "    Snapshots = Snapshots[increasing_indices]\n",
    "    print (\"modelfolder: \", modelfolder)\n",
    "    print(\"Using %s\" % Snapshots[snapshotindex], \"for model\", modelfolder)\n",
    "    dlc_cfg[\"init_weights\"] = os.path.join(\n",
    "        modelfolder, \"train\", Snapshots[snapshotindex]\n",
    "    )\n",
    "    trainingsiterations = (dlc_cfg[\"init_weights\"].split(os.sep)[-1]).split(\"-\")[-1]\n",
    "\n",
    "    # Name for scorer:\n",
    "    DLCscorer, DLCscorerlegacy = auxiliaryfunctions.GetScorerName(\n",
    "        cfg,\n",
    "        shuffle,\n",
    "        trainFraction,\n",
    "        trainingsiterations=trainingsiterations,\n",
    "        modelprefix=modelprefix,\n",
    "    )\n",
    "\n",
    "    ##################################################\n",
    "    # Looping over videos\n",
    "    ##################################################\n",
    "    Videos = auxiliaryfunctions.Getlistofvideos(videos, videotype)\n",
    "    if len(Videos) > 0:\n",
    "        for video in Videos:\n",
    "            print(\"Processing... \", video)\n",
    "            videofolder = str(Path(video).parents[0])\n",
    "            if destfolder is None:\n",
    "                destfolder = videofolder\n",
    "            auxiliaryfunctions.attempttomakefolder(destfolder)\n",
    "            vname = Path(video).stem\n",
    "            dataname = os.path.join(videofolder, vname + DLCscorer + \".h5\")\n",
    "            data, metadata = auxfun_multianimal.LoadFullMultiAnimalData(dataname)\n",
    "            method = \"sk\" if track_method == \"skeleton\" else \"bx\"\n",
    "            trackname = dataname.split(\".h5\")[0] + f\"_{method}.pickle\"\n",
    "            trackname = trackname.replace(videofolder, destfolder)\n",
    "            if (\n",
    "                os.path.isfile(trackname) and not overwrite\n",
    "            ):  # TODO: check if metadata are identical (same parameters!)\n",
    "                print(\"Tracklets already computed\", trackname)\n",
    "                print(\"Set overwrite = True to overwrite.\")\n",
    "            else:\n",
    "                print(\"Analyzing\", dataname)\n",
    "                DLCscorer = metadata[\"data\"][\"Scorer\"]\n",
    "                dlc_cfg = metadata[\"data\"][\"DLC-model-config file\"]\n",
    "                nms_radius = data[\"metadata\"][\"nms radius\"]\n",
    "                minconfidence = data[\"metadata\"][\"minimal confidence\"]\n",
    "                print (\"minconfidence: \", minconfidence)\n",
    "                #minconfidence = 0.\n",
    "\n",
    "                partaffinityfield_graph = data[\"metadata\"][\"PAFgraph\"]\n",
    "                all_joints = data[\"metadata\"][\"all_joints\"]\n",
    "                all_jointnames = data[\"metadata\"][\"all_joints_names\"]\n",
    "\n",
    "                if edgewisecondition:\n",
    "                    upperbound = np.array(\n",
    "                        [\n",
    "                            float(\n",
    "                                inferenceboundscfg[str(edge[0]) + \"_\" + str(edge[1])][\n",
    "                                    \"intra_max\"\n",
    "                                ]\n",
    "                            )\n",
    "                            for edge in partaffinityfield_graph\n",
    "                        ]\n",
    "                    )\n",
    "                    lowerbound = np.array(\n",
    "                        [\n",
    "                            float(\n",
    "                                inferenceboundscfg[str(edge[0]) + \"_\" + str(edge[1])][\n",
    "                                    \"intra_min\"\n",
    "                                ]\n",
    "                            )\n",
    "                            for edge in partaffinityfield_graph\n",
    "                        ]\n",
    "                    )\n",
    "                    upperbound *= 1.25\n",
    "                    lowerbound *= 0.5  # SLACK!\n",
    "                else:\n",
    "                    lowerbound = None\n",
    "                    upperbound = None\n",
    "\n",
    "                #lowerbound += 40\n",
    "                \n",
    "                if PAF is None:\n",
    "                    PAF = np.arange(\n",
    "                        len(partaffinityfield_graph)\n",
    "                    )  # THIS CAN BE A SUBSET!\n",
    "\n",
    "                partaffinityfield_graph = [partaffinityfield_graph[l] for l in PAF]\n",
    "                linkingpartaffinityfield_graph = partaffinityfield_graph\n",
    "\n",
    "                numjoints = len(all_jointnames)\n",
    "                if BPTS is None and iBPTS is None:\n",
    "                    # NOTE: this can be used if only a subset is relevant. I.e. [0,1] for only first and second joint!\n",
    "                    BPTS = range(numjoints)\n",
    "                    iBPTS = range(numjoints)  # the corresponding inverse!\n",
    "\n",
    "                # TODO: adjust this for multi + unique bodyparts!\n",
    "                # this is only for multianimal parts and uniquebodyparts as one (not one uniquebodyparts guy tracked etc. )\n",
    "                bodypartlabels = sum([3 * [all_jointnames[bpt]] for bpt in BPTS], [])\n",
    "                numentries = len(bodypartlabels)\n",
    "\n",
    "                scorers = numentries * [DLCscorer]\n",
    "                xylvalue = int(len(bodypartlabels) / 3) * [\"x\", \"y\", \"likelihood\"]\n",
    "                pdindex = pd.MultiIndex.from_arrays(\n",
    "                    np.vstack([scorers, bodypartlabels, xylvalue]),\n",
    "                    names=[\"scorer\", \"bodyparts\", \"coords\"],\n",
    "                )\n",
    "\n",
    "                imnames = [fn for fn in data if fn != \"metadata\"]\n",
    "\n",
    "                if track_method == \"box\":\n",
    "                    mot_tracker = trackingutils.Sort(inferencecfg)\n",
    "                else:\n",
    "                    mot_tracker = trackingutils.SORT(\n",
    "                        numjoints,\n",
    "                        inferencecfg[\"max_age\"],\n",
    "                        inferencecfg[\"min_hits\"],\n",
    "                        inferencecfg.get(\"oks_threshold\", 0.5),\n",
    "                    )\n",
    "                    \n",
    "                print (\"inferencecfg: \", inferencecfg)\n",
    "                print (\"numjoints: \", numjoints)\n",
    "                print (\"BPTS: \", BPTS)\n",
    "                print (\"iBPTS: \", iBPTS)\n",
    "                print (\"PAF: \", PAF)\n",
    "                print (\"partaffinityfield_graph: \", partaffinityfield_graph)\n",
    "                print (\"linkingpartaffinityfield_graph: \", linkingpartaffinityfield_graph)\n",
    "                \n",
    "                #print (\" data[imname]: \", data[imnames[0]])\n",
    "                print (\"lowerbound: \", lowerbound)\n",
    "                print (\"upperbound: \", upperbound)\n",
    "                print (\"printintermediate: \", printintermediate)\n",
    "                \n",
    "                Tracks = {}\n",
    "                for index, imname in tqdm(enumerate(imnames)):\n",
    "                    animals = inferenceutils.assemble_individuals(\n",
    "                        inferencecfg,\n",
    "                        data[imname],\n",
    "                        numjoints,\n",
    "                        BPTS,\n",
    "                        iBPTS,\n",
    "                        PAF,\n",
    "                        partaffinityfield_graph,\n",
    "                        linkingpartaffinityfield_graph,\n",
    "                        lowerbound,\n",
    "                        upperbound,\n",
    "                        printintermediate,\n",
    "                    )\n",
    "                    print (\"Animals: \", animals)\n",
    "                    if track_method == \"box\":\n",
    "                        # get corresponding bounding boxes!\n",
    "                        bb = inferenceutils.individual2boundingbox(\n",
    "                            inferencecfg, animals, 0\n",
    "                        )  # TODO: get cropping parameters and utilize!\n",
    "                        trackers = mot_tracker.update(bb)\n",
    "                    else:\n",
    "                        temp = [arr.reshape((-1, 3))[:, :2] for arr in animals]\n",
    "                        trackers = mot_tracker.track(temp)\n",
    "                    trackingutils.fill_tracklets(Tracks, trackers, animals, imname)\n",
    "\n",
    "                    # Test whether the unique bodyparts have been assembled\n",
    "                    # TODO Perhaps easier to check whether links were defined in the PAF graph?\n",
    "                    inds_unique = [\n",
    "                        all_jointnames.index(bp) for bp in cfg[\"uniquebodyparts\"]\n",
    "                    ]\n",
    "                    if not any(\n",
    "                        np.isfinite(a.reshape((-1, 3))[inds_unique]).all()\n",
    "                        for a in animals\n",
    "                    ):\n",
    "                        single = np.full((numjoints, 3), np.nan)\n",
    "                        single_dets = inferenceutils.convertdetectiondict2listoflist(\n",
    "                            data[imname], inds_unique\n",
    "                        )\n",
    "                        for ind, dets in zip(inds_unique, single_dets):\n",
    "                            if len(dets) == 1:\n",
    "                                single[ind] = dets[0][:3]\n",
    "                            elif len(dets) > 1:\n",
    "                                best = sorted(dets, key=lambda x: x[2], reverse=True)[0]\n",
    "                                single[ind] = best[:3]\n",
    "                        # Find an unused tracklet ID for the 'unique' bodyparts\n",
    "                        tracklet_id = 0\n",
    "                        while True:\n",
    "                            if tracklet_id not in Tracks:\n",
    "                                break\n",
    "                            tracklet_id += 1\n",
    "                        Tracks[tracklet_id] = {}\n",
    "                        Tracks[tracklet_id][imname] = single.flatten()\n",
    "\n",
    "                Tracks[\"header\"] = pdindex\n",
    "                with open(trackname, \"wb\") as f:\n",
    "                    # Pickle the 'labeled-data' dictionary using the highest protocol available.\n",
    "                    pickle.dump(Tracks, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        os.chdir(str(start_path))\n",
    "\n",
    "        print(\"The tracklets were created. Now you can 'refine_tracklets'.\")\n",
    "        # print(\"If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract any outlier frames!\")\n",
    "    else:\n",
    "        print(\"No video(s) found. Please check your path!\")\n",
    "\n",
    "                \n",
    "config = '/media/cat/14TB/insync_cm5636/march_2/video/dlc_2.26b_results/march_2_redo-cat-2020-06-17/config.yaml'\n",
    "videos = ['/media/cat/14TB/insync_cm5636/march_2/video/dlc_2.26b_results/march_2_redo-cat-2020-06-17/videos/new/2020-3-8_12_08_57_943006_compressed_3min_same.avi']\n",
    "DLCscorername = 'DLC_resnet50_march_2_redoJun17shuffle1_50000'\n",
    "destfolder = '/media/cat/14TB/insync_cm5636/march_2/video/dlc_2.26b_results/march_2_redo-cat-2020-06-17/videos/new/'   \n",
    "\n",
    "from deeplabcut.pose_estimation_tensorflow.config import load_config\n",
    "#from deeplabcut.pose_estimation_tensorflow.nnet import predict\n",
    "import pickle, re\n",
    "from deeplabcut import auxiliaryfunctions\n",
    "from pathlib import Path\n",
    "from tqdm import trange\n",
    "from skimage.draw import circle, line_aa\n",
    "from tqdm import tqdm\n",
    "\n",
    "convert_detections2tracklets(config,\n",
    "    videos,\n",
    "    videotype=\"avi\",\n",
    "    shuffle=1,\n",
    "    trainingsetindex=0,\n",
    "    overwrite=False,\n",
    "    destfolder=None,\n",
    "    BPTS=None,\n",
    "    iBPTS=None,\n",
    "    PAF=None,\n",
    "    printintermediate=False,\n",
    "    inferencecfg=None,\n",
    "    modelprefix=\"\",\n",
    "    track_method=\"box\",\n",
    "    edgewisecondition=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "([array([[540.01 ,  59.952],\n",
      "       [876.147,  92.054],\n",
      "       [235.757, 156.129],\n",
      "       [668.108, 363.838],\n",
      "       [804.761, 413.655],\n",
      "       [683.492, 684.073],\n",
      "       [523.775, 724.99 ]]), array([[859.807,  75.684],\n",
      "       [700.001, 396.363],\n",
      "       [811.655, 404.221],\n",
      "       [563.866, 764.066]]), array([[251.796, 171.921],\n",
      "       [676.316, 380.121],\n",
      "       [779.547, 412.066]]), array([[1148.23 ,  132.131],\n",
      "       [ 227.764,  188.264],\n",
      "       [ 800.673,  371.205],\n",
      "       [ 691.798,  395.77 ],\n",
      "       [1276.178,  604.042],\n",
      "       [ 683.896,  700.   ],\n",
      "       [ 564.498,  752.97 ]]), array([[1155.923,  156.068],\n",
      "       [ 257.673,  180.339],\n",
      "       [ 675.923,  387.895],\n",
      "       [ 772.544,  401.262],\n",
      "       [1276.019,  595.98 ],\n",
      "       [ 611.958,  692.293],\n",
      "       [ 676.004,  691.553],\n",
      "       [ 540.241,  714.865]]), array([[1139.629,  147.527],\n",
      "       [ 241.325,  171.161],\n",
      "       [ 791.045,  394.008],\n",
      "       [ 692.198,  395.519],\n",
      "       [ 643.481,  692.671],\n",
      "       [ 549.063,  735.228]]), array([[1124.147,  131.773],\n",
      "       [ 243.161,  194.283],\n",
      "       [ 776.485,  373.29 ],\n",
      "       [ 572.811,  714.316]]), array([[1075.049,  124.646],\n",
      "       [ 243.023,  202.232],\n",
      "       [ 762.698,  354.172],\n",
      "       [ 603.689,  700.74 ]]), array([[ 891.697,   99.89 ],\n",
      "       [1044.815,  131.373],\n",
      "       [ 243.804,  220.98 ],\n",
      "       [ 755.561,  323.197],\n",
      "       [ 641.816,  684.458]]), array([[1092.078,   99.903],\n",
      "       [ 883.766,  107.882],\n",
      "       [1002.313,  132.104],\n",
      "       [ 771.741,  307.726],\n",
      "       [ 676.415,  689.82 ]]), array([[875.885, 108.08 ],\n",
      "       [979.681, 124.438],\n",
      "       [828.092, 292.034],\n",
      "       [716.001, 395.876],\n",
      "       [700.818, 692.861],\n",
      "       [515.835, 724.194]]), array([[563.493,  75.678],\n",
      "       [876.241, 107.962],\n",
      "       [956.387, 107.896],\n",
      "       [843.601, 275.946],\n",
      "       [707.745, 396.551],\n",
      "       [716.372, 709.423]]), array([[556.217, 100.103],\n",
      "       [923.616,  99.414],\n",
      "       [843.999, 268.171],\n",
      "       [700.108, 396.216],\n",
      "       [499.972, 716.118],\n",
      "       [739.212, 731.988]]), array([[ 555.965,   59.938],\n",
      "       [ 868.654,   66.597],\n",
      "       [ 555.829,  131.979],\n",
      "       [ 852.128,  251.872],\n",
      "       [ 699.935,  396.011],\n",
      "       [1019.893,  587.838],\n",
      "       [ 699.606,  699.844],\n",
      "       [ 748.571,  773.185]])],)\n"
     ]
    }
   ],
   "source": [
    "csv_fname = '/media/cat/14TB/insync_cm5636/march_2/video/dlc_2.26b_results/march_2_redo-cat-2020-06-17/videos/new/2020-3-8_12_08_57_943006_compressed_3min_sameDLC_resnet50_march_2_redoJun17shuffle1_50000_bx_filtered.csv'\n",
    "table = csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500, 3)\n",
      "tracex:  (56, 4500)\n",
      "tracey:  (56, 4500)\n",
      "likelihood:  (56, 4500)\n",
      "(56, 4500) 0\n",
      "(56, 4500) 1\n",
      "(56, 4500) 2\n",
      "(56, 4500) 3\n",
      "(56, 4500) 4\n",
      "(56, 4500) 5\n",
      "(56, 4500) 6\n",
      "(56, 4500) 7\n",
      "(56, 4500) 8\n",
      "(56, 4500) 9\n",
      "(56, 4500) 10\n",
      "(56, 4500) 11\n",
      "(56, 4500) 12\n",
      "(56, 4500) 13\n",
      "(56, 4500) 14\n",
      "(56, 4500) 15\n",
      "(56, 4500) 16\n",
      "(56, 4500) 17\n",
      "(56, 4500) 18\n",
      "(56, 4500) 19\n",
      "(56, 4500) 20\n",
      "(56, 4500) 21\n",
      "(56, 4500) 22\n",
      "(56, 4500) 23\n",
      "(56, 4500) 24\n",
      "(56, 4500) 25\n",
      "(56, 4500) 26\n",
      "(56, 4500) 27\n",
      "(56, 4500) 28\n",
      "(56, 4500) 29\n",
      "(56, 4500) 30\n",
      "(56, 4500) 31\n",
      "(56, 4500) 32\n",
      "(56, 4500) 33\n",
      "(56, 4500) 34\n",
      "(56, 4500) 35\n",
      "(56, 4500) 36\n",
      "(56, 4500) 37\n",
      "(56, 4500) 38\n",
      "(56, 4500) 39\n",
      "(56, 4500) 40\n",
      "(56, 4500) 41\n",
      "(56, 4500) 42\n",
      "(56, 4500) 43\n",
      "(56, 4500) 44\n",
      "(56, 4500) 45\n",
      "(56, 4500) 46\n",
      "(56, 4500) 47\n",
      "(56, 4500) 48\n",
      "(56, 4500) 49\n",
      "(56, 4500) 50\n",
      "(56, 4500) 51\n",
      "(56, 4500) 52\n",
      "(56, 4500) 53\n",
      "(56, 4500) 54\n",
      "(56, 4500) 55\n"
     ]
    }
   ],
   "source": [
    "# CONVERT THE TABLE VALUES TO X,Y and LIKELIHOOD LOCATIONS;\n",
    "print (traces[0].shape)\n",
    "table = traces.copy()\n",
    "tracesx = []\n",
    "tracesy = []\n",
    "likelihoods = []\n",
    "for k in range(0,len(traces),1):\n",
    "    tracesx.append(traces[k][:,0])\n",
    "    tracesy.append(traces[k][:,1])\n",
    "    likelihoods.append(traces[k][:,2])\n",
    "\n",
    "likelihoods = np.array(likelihoods)\n",
    "tracesx = np.array(tracesx)\n",
    "tracesy = np.array(tracesy)\n",
    "print (\"tracex: \", tracesx.shape)\n",
    "print (\"tracey: \", tracesy.shape)\n",
    "print (\"likelihood: \", likelihoods.shape)\n",
    "\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "cmap = cm.get_cmap('viridis',likelihoods.shape[0])\n",
    "\n",
    "# OPTIONAL VISUALIZE FEATURES AS SCATTER PLOTS\n",
    "for feature in range(len(traces)):\n",
    "    print (likelihoods.shape, feature)\n",
    "    clrs = cmap(np.arange(likelihoods.shape[1]))*likelihoods[feature][:,None]\n",
    "    ax=plt.subplot(7,8,feature+1)\n",
    "    plt.scatter(tracesx[feature], tracesy[feature], color=clrs)\n",
    "    \n",
    "    plt.ylim(0,1024)\n",
    "    plt.xlim(0,1280)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD VIDE TO ANNOTATE\n",
    "video_name = \"/media/cat/14TB/insync_cm5636/march_2/video/dlc_2.26b_results/march_2_redo-cat-2020-06-17/videos/new/2020-3-8_12_08_57_943006_compressed_3min_same.avi\"\n",
    "frame_rate = 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n"
     ]
    }
   ],
   "source": [
    "# video writing step\n",
    "#video_out.release()\n",
    "#out.release()\n",
    "#original_vid.release()\n",
    "#cv2.destroyAllWindows()\n",
    "        \n",
    "original_vid = cv2.VideoCapture(video_name)\n",
    "#cap = cv2.VideoCapture('chaplin.mp4')\n",
    "\n",
    "ctr=0\n",
    "# figure out index of starting frame and move video to location\n",
    "frame_no1 = 0 #56*60*frame_rate\n",
    "original_vid.set(1,frame_no1)\n",
    "\n",
    "# initialize video writer for file to be saved\n",
    "out_dir = '/media/cat/14TB/insync_cm5636/march_2/video/dlc_2.26b_results/march_2_redo-cat-2020-06-17/videos/new/'\n",
    "fname_out = out_dir+'video_labeled.avi'\n",
    "fourcc = cv2.VideoWriter_fourcc('M','P','E','G')\n",
    "# fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "# fourCc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "video_out = cv2.VideoWriter(fname_out,fourcc, 25, (1280,1024), True)\n",
    "\n",
    "# Read the next frame from the video. If you set frame 749 above then the code will return the last frame.\n",
    "n_sec = 180\n",
    "clrs = ['blue','red','yellow','green']\n",
    "\n",
    "# \n",
    "for k in range(tracesx.shape[1]):    \n",
    "    if k %100==0:\n",
    "        print (k)\n",
    "    ret, frame = original_vid.read()\n",
    "    if ret==False:\n",
    "        break\n",
    "\n",
    "    frame_cropped = frame.copy()#[:,150:1174][::2,::2]\n",
    "\n",
    "    for p in range(tracesx.shape[0]):\n",
    "        x = np.int32(tracesx[p,k])\n",
    "        y = np.int32(tracesy[p,k])\n",
    "        #frame_cropped[y[k]-5:y[k]+5,x[k]-5:x[k]+5]= (np.float32(\n",
    "        #    matplotlib.colors.to_rgb(clrs[p//4]))*255.).astype('uint8')\n",
    "        frame_cropped[y-5:y+5,x-5:x+5]= (np.float32(\n",
    "            matplotlib.colors.to_rgb(clrs[p//14]))*255.).astype('uint8')\n",
    "\n",
    "    #cv2.imshow('frame',frame_cropped)\n",
    "    #if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    #   break\n",
    "\n",
    "    video_out.write(frame_cropped)\n",
    "    #out.write(frame_cropped)\n",
    "\n",
    "ctr+=1\n",
    "\n",
    "video_out.release()\n",
    "original_vid.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "#break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'A-Male adult', 'B-Female adult', 'C-fluffy pup',\n",
      "       'D-shaved pup', 'Unnamed: 5', 'Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8',\n",
      "       'Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "df = pandas.read_excel('/media/cat/1TB/insync_cm5635/march_2/video/completed/2020-3-15_03:51:56:415333_compressed/2020-3-15_03_51_56_415333.xlsx')\n",
    "\n",
    "# print the column names\n",
    "print (df.columns)\n",
    "\n",
    "# get the values for a given column\n",
    "times_ = df[df.columns[0]].values\n",
    "time_array = []\n",
    "for t in times_:\n",
    "    time_array.append((t.hour * 60 + t.minute) * 60 + t.second)\n",
    "\n",
    "male = df[df.columns[1]].values\n",
    "female = df[df.columns[2]].values\n",
    "pup1 = df[df.columns[3]].values\n",
    "pup2 = df[df.columns[4]].values\n",
    "\n",
    "# \n",
    "idx = np.where(male=='A-C')[0]\n",
    "male[idx]=13\n",
    "idx = np.where(male=='A-B')[0]\n",
    "male[idx]=13\n",
    "\n",
    "idx = np.where(female=='B-D')[0]\n",
    "female[idx]=13\n",
    "idx = np.where(female=='B-C')[0]\n",
    "female[idx]=13\n",
    "\n",
    "idx = np.where(pup1=='B-C')[0]\n",
    "pup1[idx]=13\n",
    "idx = np.where(pup1=='C-D')[0]\n",
    "pup1[idx]=13\n",
    "idx = np.where(pup1=='A-C')[0]\n",
    "pup1[idx]=13\n",
    "\n",
    "idx = np.where(pup2=='A-C')[0]\n",
    "pup2[idx]=13\n",
    "idx = np.where(pup2=='A-B')[0]\n",
    "pup2[idx]=13\n",
    "idx = np.where(pup2=='C-D')[0]\n",
    "pup2[idx]=13\n",
    "idx = np.where(pup2=='A-D')[0]\n",
    "pup2[idx]=13\n",
    "idx = np.where(pup2=='B-D')[0]\n",
    "pup2[idx]=13\n",
    "#\n",
    "plt.plot(male, c='blue', label=\"male\")\n",
    "plt.plot(female, c='red', label=\"female\")\n",
    "plt.plot(pup1, c='green', label=\"pup1\")\n",
    "plt.plot(pup2, c='cyan', label=\"pup2\")\n",
    "plt.xlabel(\"Time (sec)\", fontsize=20)\n",
    "plt.ylabel(\"States\",fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "\n",
    "plt.scatter(start, start*0+13, c='black')\n",
    "\n",
    "labels = [\n",
    "    \"1 Scratching walls\",\n",
    "    \"2 Digging beddings\",\n",
    "    \"3 Playing (scratching) with blocks or hut\",\n",
    "    \"4 Self grooming\",\n",
    "    \"5 Sleeping\",\n",
    "    \"6 Walking\",\n",
    "    \"7 Chasing\",\n",
    "    \"8 Being Chased\",\n",
    "    \"9 Eating\",\n",
    "    \"10 Drinking/ at spout\",\n",
    "    \"11 Occluded\",\n",
    "    \"12\",\n",
    "    \"13 Social Interaction\"\n",
    "]\n",
    "plt.yticks(np.arange(1,14,1),labels)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data labels:  ['individuals', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_female', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'adult_male', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_shaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved', 'pup_unshaved']\n",
      "column vals:  ['bodyparts', 'nose', 'nose', 'nose', 'lefteye', 'lefteye', 'lefteye', 'righteye', 'righteye', 'righteye', 'leftear', 'leftear', 'leftear', 'rightear', 'rightear', 'rightear', 'spine1', 'spine1', 'spine1', 'spine2', 'spine2', 'spine2', 'spine3', 'spine3', 'spine3', 'spine4', 'spine4', 'spine4', 'spine5', 'spine5', 'spine5', 'tail1', 'tail1', 'tail1', 'tail2', 'tail2', 'tail2', 'tail3', 'tail3', 'tail3', 'tail4', 'tail4', 'tail4', 'nose', 'nose', 'nose', 'lefteye', 'lefteye', 'lefteye', 'righteye', 'righteye', 'righteye', 'leftear', 'leftear', 'leftear', 'rightear', 'rightear', 'rightear', 'spine1', 'spine1', 'spine1', 'spine2', 'spine2', 'spine2', 'spine3', 'spine3', 'spine3', 'spine4', 'spine4', 'spine4', 'spine5', 'spine5', 'spine5', 'tail1', 'tail1', 'tail1', 'tail2', 'tail2', 'tail2', 'tail3', 'tail3', 'tail3', 'tail4', 'tail4', 'tail4', 'nose', 'nose', 'nose', 'lefteye', 'lefteye', 'lefteye', 'righteye', 'righteye', 'righteye', 'leftear', 'leftear', 'leftear', 'rightear', 'rightear', 'rightear', 'spine1', 'spine1', 'spine1', 'spine2', 'spine2', 'spine2', 'spine3', 'spine3', 'spine3', 'spine4', 'spine4', 'spine4', 'spine5', 'spine5', 'spine5', 'tail1', 'tail1', 'tail1', 'tail2', 'tail2', 'tail2', 'tail3', 'tail3', 'tail3', 'tail4', 'tail4', 'tail4', 'nose', 'nose', 'nose', 'lefteye', 'lefteye', 'lefteye', 'righteye', 'righteye', 'righteye', 'leftear', 'leftear', 'leftear', 'rightear', 'rightear', 'rightear', 'spine1', 'spine1', 'spine1', 'spine2', 'spine2', 'spine2', 'spine3', 'spine3', 'spine3', 'spine4', 'spine4', 'spine4', 'spine5', 'spine5', 'spine5', 'tail1', 'tail1', 'tail1', 'tail2', 'tail2', 'tail2', 'tail3', 'tail3', 'tail3', 'tail4', 'tail4', 'tail4']\n",
      "# of datapoints (x,y,likelihood):  (4501, 169)\n",
      "1 TEMP:  (4500, 3)\n",
      "4 TEMP:  (4500, 3)\n",
      "7 TEMP:  (4500, 3)\n",
      "10 TEMP:  (4500, 3)\n",
      "13 TEMP:  (4500, 3)\n",
      "16 TEMP:  (4500, 3)\n",
      "19 TEMP:  (4500, 3)\n",
      "22 TEMP:  (4500, 3)\n",
      "25 TEMP:  (4500, 3)\n",
      "28 TEMP:  (4500, 3)\n",
      "31 TEMP:  (4500, 3)\n",
      "34 TEMP:  (4500, 3)\n",
      "37 TEMP:  (4500, 3)\n",
      "40 TEMP:  (4500, 3)\n",
      "43 TEMP:  (4500, 3)\n",
      "46 TEMP:  (4500, 3)\n",
      "49 TEMP:  (4500, 3)\n",
      "52 TEMP:  (4500, 3)\n",
      "55 TEMP:  (4500, 3)\n",
      "58 TEMP:  (4500, 3)\n",
      "61 TEMP:  (4500, 3)\n",
      "64 TEMP:  (4500, 3)\n",
      "67 TEMP:  (4500, 3)\n",
      "70 TEMP:  (4500, 3)\n",
      "73 TEMP:  (4500, 3)\n",
      "76 TEMP:  (4500, 3)\n",
      "79 TEMP:  (4500, 3)\n",
      "82 TEMP:  (4500, 3)\n",
      "85 TEMP:  (4500, 3)\n",
      "88 TEMP:  (4500, 3)\n",
      "91 TEMP:  (4500, 3)\n",
      "94 TEMP:  (4500, 3)\n",
      "97 TEMP:  (4500, 3)\n",
      "100 TEMP:  (4500, 3)\n",
      "103 TEMP:  (4500, 3)\n",
      "106 TEMP:  (4500, 3)\n",
      "109 TEMP:  (4500, 3)\n",
      "112 TEMP:  (4500, 3)\n",
      "115 TEMP:  (4500, 3)\n",
      "118 TEMP:  (4500, 3)\n",
      "121 TEMP:  (4500, 3)\n",
      "124 TEMP:  (4500, 3)\n",
      "127 TEMP:  (4500, 3)\n",
      "130 TEMP:  (4500, 3)\n",
      "133 TEMP:  (4500, 3)\n",
      "136 TEMP:  (4500, 3)\n",
      "139 TEMP:  (4500, 3)\n",
      "142 TEMP:  (4500, 3)\n",
      "145 TEMP:  (4500, 3)\n",
      "148 TEMP:  (4500, 3)\n",
      "151 TEMP:  (4500, 3)\n",
      "154 TEMP:  (4500, 3)\n",
      "157 TEMP:  (4500, 3)\n",
      "160 TEMP:  (4500, 3)\n",
      "163 TEMP:  (4500, 3)\n",
      "166 TEMP:  (4500, 3)\n"
     ]
    }
   ],
   "source": [
    "# DLC csv labeling frame; OLD METHOD USING .CSV FILES\n",
    "fname = '/media/cat/14TB/insync_cm5636/march_2/video/dlc_2.26b_results/march_2_redo-cat-2020-06-17/videos/new/2020-3-8_12_08_57_943006_compressed_3min_sameDLC_resnet50_march_2_redoJun17shuffle1_50000_bx_filtered.csv'\n",
    "\n",
    "def load_csv(fname):\n",
    "    with open(fname, newline='') as csvfile:\n",
    "        data = list(csv.reader(csvfile))\n",
    "\n",
    "    labels = data[1]\n",
    "    print (\"data labels: \", labels)\n",
    "    print (\"column vals: \", data[2])\n",
    "\n",
    "    # load values\n",
    "    data_array = np.array(data[3:])\n",
    "    print (\"# of datapoints (x,y,likelihood): \", data_array.shape)\n",
    "\n",
    "    # \n",
    "    #labels = ['fnose','f_leye','f_reye','f_lear','f_rear','f_',\n",
    "    #         'male_nose','male_left_ear','male_right_ear','male_base_of_tail',\n",
    "    #          'pup_shaved_nose','pup_shaved_left_ear','pup_shaved_right_ear','pup_shaved_base_of_tail',\n",
    "    #          'pup_noshave_nose','pup_noshave_left_ear','pup_noshave_right_ear','pup_noshave_base_of_tail'             \n",
    "    #         ]\n",
    "    \n",
    "    labels = labels[1:]\n",
    "    \n",
    "    traces = []\n",
    "    traces_nan = []\n",
    "    # zero out low quality DLC values\n",
    "    for idx in range(1,len(labels)-1,3):\n",
    "        #print (\"idx: \", idx)\n",
    "        #print (data_array[1:,idx:idx+3])\n",
    "        #print (data_array[1:,idx:idx+3].shape)\n",
    "\n",
    "        temp = data_array[1:,idx:idx+3]\n",
    "        idx1 = np.where(temp=='')[0]\n",
    "        temp[idx1]=0\n",
    "        temp = temp.astype(np.float)# np.array(temp)\n",
    "        print (idx, \"TEMP: \", temp.shape)\n",
    "        #temp.replace(\"''\",'0')\n",
    "        \n",
    "\n",
    "        # replace low likelihoods with median\n",
    "        likelihoods = temp[:,2]\n",
    "        idx2 = np.where(likelihoods<0.8)[0]\n",
    "        temp[idx2,0]=np.median(temp[:,0])\n",
    "        temp[idx2,1]=np.median(temp[:,1])\n",
    "        traces.append(temp.copy())\n",
    "        \n",
    "        temp[idx2,0]=np.nan\n",
    "        temp[idx2,1]=np.nan\n",
    "        traces_nan.append(temp.copy())\n",
    "\n",
    "    return traces, labels, traces_nan\n",
    "traces, labels, traces_nan = load_csv(fname)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-fed9a04fd964>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#              np.sum(old_frame[:,:,1]-frame[:,:,1]),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#              np.sum(old_frame[:,:,2]-frame[:,:,2])])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mmotion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_frame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# plot time course of video vs. other data \n",
    "fname_vid = '/media/cat/1TB/insync_cm5635/march_2/video/completed/dan_USV_triggered/2020-3-15_03_51_56_415333/2020-3-15_03:51:56:415333_compressed.avi'\n",
    "original_vid.release()\n",
    "original_vid = cv2.VideoCapture(video_name)\n",
    "original_vid.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "\n",
    "ctr=0\n",
    "motion = []\n",
    "ret, old_frame = original_vid.read()\n",
    "while True:\n",
    "    if ctr%1000==0:\n",
    "        print (ctr)\n",
    "\n",
    "    ret, frame = original_vid.read()\n",
    "    \n",
    "    #motion.append([np.sum(old_frame[:,:,0]-frame[:,:,0]),\n",
    "    #              np.sum(old_frame[:,:,1]-frame[:,:,1]),\n",
    "    #              np.sum(old_frame[:,:,2]-frame[:,:,2])])\n",
    "    motion.append(np.sum(old_frame[:,:,1]-frame[:,:,1]))\n",
    "    \n",
    "    if ret==False:\n",
    "        break\n",
    "    #if ctr>1000:\n",
    "    #    break\n",
    "    ctr+=1\n",
    "    old_frame = frame\n",
    "        \n",
    "motion = np.vstack(motion)\n",
    "print (motion)\n",
    "print (motion.shape)\n",
    "fig=plt.figure()\n",
    "plt.plot(motion,c='green')\n",
    "# plt.plot(motion[:,1],c='green')\n",
    "# plt.plot(motion[:,2],c='blue')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89987,)\n"
     ]
    }
   ],
   "source": [
    "print (np.array(motion).shape)\n",
    "\n",
    "np.save('/media/cat/1TB/insync_cm5635/march_2/video/completed/2020-3-15_03:51:56:415333_compressed/motion.npy', motion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00000000e+00 2.77817907e-04 5.55635814e-04 ... 1.24993055e+01\n",
      " 1.24995833e+01 1.24998611e+01]\n",
      "(44994,)\n",
      "(44994,)\n"
     ]
    }
   ],
   "source": [
    "fs=25\n",
    "\n",
    "f, Pxx_den = signal.periodogram(np.float32(motion).squeeze(), fs)\n",
    "print (f)\n",
    "print (f.shape)\n",
    "print (Pxx_den.shape)\n",
    "plt.semilogy(f, Pxx_den)\n",
    "#plt.ylim([1e-7, 1e2])\n",
    "plt.xlabel('frequency [Hz]')\n",
    "plt.ylabel('PSD [V**2/Hz]')\n",
    "plt.title(\"Video power spectrum\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000,)\n"
     ]
    }
   ],
   "source": [
    "# filter data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 330. GiB for an array with shape (89988, 1024, 1280, 3) and data type uint8",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-20c2c8836d47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mskvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfname_vid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/media/cat/1TB/insync_cm5635/march_2/video/completed/dan_USV_triggered/2020-3-15_03_51_56_415333/2020-3-15_03:51:56:415333_compressed.avi'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvideodata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_vid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideodata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/skvideo/io/io.py\u001b[0m in \u001b[0;36mvread\u001b[0;34m(fname, height, width, num_frames, as_grey, inputdict, outputdict, backend, verbosity)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mvideodata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnextFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mvideodata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 330. GiB for an array with shape (89988, 1024, 1280, 3) and data type uint8"
     ]
    }
   ],
   "source": [
    "import skvideo.io  \n",
    "fname_vid = '/media/cat/1TB/insync_cm5635/march_2/video/completed/dan_USV_triggered/2020-3-15_03_51_56_415333/2020-3-15_03:51:56:415333_compressed.avi'\n",
    "videodata = skvideo.io.vread(fname_vid)  \n",
    "print(videodata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
